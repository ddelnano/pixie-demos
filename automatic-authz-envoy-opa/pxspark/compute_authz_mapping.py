from pyspark.sql.dataframe import DataFrame
from pyspark.sql.functions import col, collect_set, explode, first, first_value, lower, regexp_extract, udf, when
from pyspark.sql.types import StringType

# More complex URL clustering/prefix matching can be done to scope L7 authz rules
# more tightly. An example of how to extend this would be to use your services' OpenAPI
# (Swagger) specs to extract route information from the URL and use that as the prefix.
from pxspark.udfs import extract_url_prefix

IGNORE_REQ_SUBSTRS = [
    "metrics", # Ignore /metrics requests
    "health",  # Ignore /health, /healthz requests
    "read",    # Ignore /ready, /readiness requests
]

def compute_authz_service_mapping(df):
    """
    compute_authz_service_mapping expects a Dataframe created from OpenTelemetry's
    opentelemetry.proto.trace.v1.ResourceSpan protobuf message type. The OTel collector
    exports this type for trace data when using a variety of its collectors. The
    file and awss3 exporters are two examples of this.
    """
    resources_df = df.select(explode(col("resourceSpans")))
    resources_df = (resources_df
                        .withColumn("resource_attrs", explode(col("col.resource.attributes")))
                        .filter(col("resource_attrs.key") == "http.xfcc_by")
                        .withColumn("xfcc_by", col("resource_attrs.value.stringValue"))
                        .withColumn("service_name", regexp_extract('xfcc_by', r'\/(\w+)$', 1)))
    resources_df.drop("resource_attrs")
    resources_df.drop("xfcc_by")

    df_exploded_scopespans = (resources_df
        .withColumn("scopespans_exploded", explode("col.scopeSpans")))

    df_exploded_spans = (df_exploded_scopespans
        .withColumn("spans_exploded", explode("scopespans_exploded.spans"))
        .drop("scopespans_exploded"))

    df_exploded_attributes = (df_exploded_spans
        .withColumn("span_attributes", explode("spans_exploded.attributes"))
        .withColumn("spanId", col("spans_exploded.spanId"))
        .drop("spans_exploded"))

    # Group by a unique column (spanId). This is auto generated by pixie if the span didn't contain
    # a X-B3-SpanId header.
    result = (df_exploded_attributes
        .groupBy("spanId")
        .agg(
            first(when(col("span_attributes.key") == "http.xfcc_uri", col("span_attributes.value.stringValue")), ignorenulls=True).alias("xfcc_uri"),
            first(when(col("span_attributes.key") == "http.target", extract_url_prefix(col("span_attributes.value.stringValue"))), ignorenulls=True).alias("http_target"),
            first(when(col("span_attributes.key") == "http.method", col("span_attributes.value.stringValue")), ignorenulls=True).alias("http_method"),
            first(col("service_name")).alias("service_name"))
        .filter(~ lower(col("http_target")).rlike("|".join(IGNORE_REQ_SUBSTRS)))
        .select(
            col("service_name"),
            regexp_extract('xfcc_uri', r'\/(\w+)$', 1).alias("client_name"),
            col("http_target"),
            col("http_method")))

    return result.groupBy("service_name", "client_name", "http_target", "http_method")

if __name__ == "__main__":
    # TODO(ddelnano): Add main function that kicks off spark job
    pass
